{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f7f04e",
   "metadata": {},
   "source": [
    "### Spacy Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dd9b52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67b9230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm') # loading language library (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c4ca2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'Tesla is looking at buying U.S. startup for $6 million') # unicode string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21d2b3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla 96 PROPN nsubj\n",
      "is 87 AUX aux\n",
      "looking 100 VERB ROOT\n",
      "at 85 ADP prep\n",
      "buying 100 VERB pcomp\n",
      "U.S. 96 PROPN compound\n",
      "startup 92 NOUN dobj\n",
      "for 85 ADP prep\n",
      "$ 99 SYM quantmod\n",
      "6 93 NUM compound\n",
      "million 93 NUM pobj\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos, token.pos_, token.dep_) \n",
    "    # pos -> Part Of Speech\n",
    "    # dep -> syntactic dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92df4e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x289087d8f40>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x289087d8d60>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x289086e4cf0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x2890898c480>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x289089e6ac0>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x289086e4f20>)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488429a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ef415",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(u\"Tesla isn't        looking into startups anymore.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9c972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc2:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe6a06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2[0], doc2[0].pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163b012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('PROPN'), spacy.explain('nsubj')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7853c66c",
   "metadata": {},
   "source": [
    "###### Additional Token Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4689165",
   "metadata": {},
   "source": [
    "|Tag|Description|doc2[0].tag|\n",
    "|:------|:------:|:------|\n",
    "|`.text`|The original word text<!-- .element: style=\"text-align:left;\" -->|`Tesla`|\n",
    "|`.lemma_`|The base form of the word|`tesla`|\n",
    "|`.pos_`|The simple part-of-speech tag|`PROPN`/`proper noun`|\n",
    "|`.tag_`|The detailed part-of-speech tag|`NNP`/`noun, proper singular`|\n",
    "|`.shape_`|The word shape – capitalization, punctuation, digits|`Xxxxx`|\n",
    "|`.is_alpha`|Is the token an alpha character?|`True`|\n",
    "|`.is_stop`|Is the token part of a stop list, i.e. the most common words of the language?|`False`|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2663eec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmas (the base form of the word):\n",
    "print(doc2[4].text)\n",
    "print(doc2[4].lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4438f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Parts-of-Speech & Detailed Tags:\n",
    "print(doc2[4].pos_)\n",
    "print(doc2[4].tag_ + ' / ' + spacy.explain(doc2[4].tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabd5ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Shapes:\n",
    "print(doc2[0].text+': '+doc2[0].shape_)\n",
    "print(doc[5].text+' : '+doc[5].shape_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e36891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean Values:\n",
    "print(doc2[0].is_alpha)\n",
    "print(doc2[0].is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4897f2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2[3], doc2[3].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b3b171",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = nlp(u'Although commmonly attributed to John Lennon from his song \"Beautiful Boy\", \\\n",
    "the phrase \"Life is what happens to us while we are making other plans\" was written by \\\n",
    "cartoonist Allen Saunders and published in Reader\\'s Digest in 1957, when Lennon was 17.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986b555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# span is a slice of a doc object\n",
    "life_quote = doc3[16:30]\n",
    "print(life_quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de207786",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(doc3), type(life_quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f7687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4 = nlp(u\"This is the first sentence. This is another sentence. This is the last sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81854688",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in doc4.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a02dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4[6], doc4[6].is_sent_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed32534",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4[8], doc4[8].is_sent_start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba3bef4",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37e291e",
   "metadata": {},
   "source": [
    "-  **Prefix**:\tCharacter(s) at the beginning &#9656; `$ ( “ ¿`\n",
    "-  **Suffix**:\tCharacter(s) at the end &#9656; `km ) , . ! ”`\n",
    "-  **Infix**:\tCharacter(s) in between &#9656; `- -- / ...`\n",
    "-  **Exception**: Special-case rule to split a string into several tokens or prevent a token from being split when punctuation rules are applied &#9656; `St. U.S.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbcb296",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystring = '\"We\\'re moving to L.A.!\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a725f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5db4516",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(mystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1666ecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a110db",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(u\"We're here to help! Send snail-mail, email support@oursite.com or visit us at http://www.oursite.com!\")\n",
    "\n",
    "for t in doc2:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f19ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = nlp(u'A 5km NYC cab ride costs $10.30')\n",
    "\n",
    "for t in doc3:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6637302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4 = nlp(u\"Let's visit St. Louis in the U.S. next year.\")\n",
    "\n",
    "for t in doc4:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b305dc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc4) # number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e806f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when we load it up en_core_web_sm that has a vocabulary of 794 different types of tokens\n",
    "len(doc4.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255bc29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc5 = nlp(u'It is better to give than to receive.')\n",
    "\n",
    "# Retrieve the third token:\n",
    "doc5[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26c9b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve three tokens from the middle:\n",
    "doc5[2:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74e3b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the last four tokens:\n",
    "doc5[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ba5013",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc6 = nlp(u'My dinner was horrible.')\n",
    "doc7 = nlp(u'Your dinner was delicious.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a993a2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc6[3] = doc7[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecf6492",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc8 = nlp(u'Apple to build a Hong Kong factory for $6 million')\n",
    "\n",
    "for token in doc8:\n",
    "    print(token.text, end=' | ')\n",
    "\n",
    "print('\\n----')\n",
    "\n",
    "# named entities\n",
    "for ent in doc8.ents:\n",
    "    print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4676838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun chunks -> Noun + the words describing the noun\n",
    "\n",
    "doc9 = nlp(u\"Autonomous cars shift insurance liability toward manufacturers.\")\n",
    "\n",
    "for chunk in doc9.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e49013",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc10 = nlp(u\"Red cars do not carry higher insurance rates.\")\n",
    "\n",
    "for chunk in doc10.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c7b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc11 = nlp(u\"He was a one-eyed, one-horned, flying, purple people-eater.\")\n",
    "\n",
    "for chunk in doc11.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003c5877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72865a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"Apple is going to build a U.K. factory for $6 million.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c68c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"dep\", jupyter=True, options={'distance': 125})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32631433",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"Over the last quarter Apple sold nearly 20 thousand iPods for a profit of $6 million.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a959c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\", jupyter=True, options={'distance': 125})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2b000a",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e848d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e808b61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f4b2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4931df36",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['run', 'runner', 'ran', 'runs', 'easily', 'fairly', 'fairness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea2f59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    print(word + ' ------> ' + p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33660ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b751943",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735c9559",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    print(word + ' ------> ' + s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a7a886",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['generous', 'generously', 'generation', 'generate']\n",
    "for word in words:\n",
    "    print(word + ' ------> ' + s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c615560",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = 'I am meeting him tomorrow at the meeting'\n",
    "for word in phrase.split():\n",
    "    print(word+' ------> '+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f703f8fd",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ec0ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(u\"I am a runner running in a race because I love to run since I ran today\")\n",
    "\n",
    "for token in doc1:\n",
    "    print(token.text, '\\t', token.pos_, '\\t', token.lemma, '\\t', token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7cae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lemmas(text):\n",
    "    for token in text:\n",
    "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946b2816",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(u\"I saw eighteen mice today!\")\n",
    "\n",
    "show_lemmas(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f404ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = nlp(u\"I am meeting him tomorrow at the meeting.\")\n",
    "\n",
    "show_lemmas(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c7330",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4 = nlp(u\"That's an enormous automobile\")\n",
    "\n",
    "show_lemmas(doc4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f49e154",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0241dc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34730be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6065662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['myself'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de19befe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['mystery'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5b5931",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['is']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbc578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.stop_words.add('btw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d09cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['btw'].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010c4058",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.stop_words.remove('beyond')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673d0a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['beyond'].is_stop = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aefa426",
   "metadata": {},
   "source": [
    "### Phrase Matching and  Vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef969de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b719d481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SolarPower\n",
    "pattern1 = [{'LOWER': 'solarpower'}] # transform word to lowercase and then does it match 'solarpower' ?\n",
    "\n",
    "# Solar Power\n",
    "pattern2 = [{'LOWER': 'solar'}, {'LOWER': 'power'}]\n",
    "\n",
    "# Solar-Power\n",
    "pattern3 = [{'LOWER': 'solar'}, {'IS_PUNCT': True}, {'LOWER': 'power'}]\n",
    "\n",
    "matcher.add('SolarPower', [pattern1, pattern2, pattern3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a543afcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'The Solar Power industry continues to grow as demand \\\n",
    "for solarpower increases. Solar-power cars are gaining popularity. Solar--power is solarpower yay!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a287bc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8656102463236116519, 1, 3), (8656102463236116519, 10, 11), (8656102463236116519, 13, 16), (8656102463236116519, 21, 24), (8656102463236116519, 25, 26)]\n"
     ]
    }
   ],
   "source": [
    "found_matches = matcher(doc)\n",
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "013c498e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8656102463236116519 SolarPower 1 3 Solar Power\n",
      "8656102463236116519 SolarPower 10 11 solarpower\n",
      "8656102463236116519 SolarPower 13 16 Solar-power\n",
      "8656102463236116519 SolarPower 21 24 Solar--power\n",
      "8656102463236116519 SolarPower 25 26 solarpower\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in found_matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
    "    span = doc[start:end]                    # get the matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a3582a",
   "metadata": {},
   "source": [
    "The following quantifiers can be passed to the `'OP'` key:\n",
    "<table><tr><th>OP</th><th>Description</th></tr>\n",
    "\n",
    "<tr ><td><span >\\!</span></td><td>Negate the pattern, by requiring it to match exactly 0 times</td></tr>\n",
    "<tr ><td><span >?</span></td><td>Make the pattern optional, by allowing it to match 0 or 1 times</td></tr>\n",
    "<tr ><td><span >\\+</span></td><td>Require the pattern to match 1 or more times</td></tr>\n",
    "<tr ><td><span >\\*</span></td><td>Allow the pattern to match zero or more times</td></tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62b69f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern1 = [{'LOWER': 'solarpower'}]\n",
    "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'power'}]\n",
    "\n",
    "matcher.remove('SolarPower')\n",
    "\n",
    "matcher.add('SolarPower', [pattern1, pattern2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08bdb696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8656102463236116519, 1, 3), (8656102463236116519, 10, 11), (8656102463236116519, 13, 16), (8656102463236116519, 21, 24), (8656102463236116519, 25, 26)]\n"
     ]
    }
   ],
   "source": [
    "found_matches = matcher(doc)\n",
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1da85ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u'Solar-powered energy runs solar-powered cars.')\n",
    "found_matches = matcher(doc2)\n",
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a438431b",
   "metadata": {},
   "source": [
    "Besides lemmas, there are a variety of token attributes we can use to determine matching rules:\n",
    "<table><tr><th>Attribute</th><th>Description</th></tr>\n",
    "\n",
    "<tr ><td><span >`ORTH`</span></td><td>The exact verbatim text of a token</td></tr>\n",
    "<tr ><td><span >`LOWER`</span></td><td>The lowercase form of the token text</td></tr>\n",
    "<tr ><td><span >`LENGTH`</span></td><td>The length of the token text</td></tr>\n",
    "<tr ><td><span >`IS_ALPHA`, `IS_ASCII`, `IS_DIGIT`</span></td><td>Token text consists of alphanumeric characters, ASCII characters, digits</td></tr>\n",
    "<tr ><td><span >`IS_LOWER`, `IS_UPPER`, `IS_TITLE`</span></td><td>Token text is in lowercase, uppercase, titlecase</td></tr>\n",
    "<tr ><td><span >`IS_PUNCT`, `IS_SPACE`, `IS_STOP`</span></td><td>Token is punctuation, whitespace, stop word</td></tr>\n",
    "<tr ><td><span >`LIKE_NUM`, `LIKE_URL`, `LIKE_EMAIL`</span></td><td>Token text resembles a number, URL, email</td></tr>\n",
    "<tr ><td><span >`POS`, `TAG`, `DEP`, `LEMMA`, `SHAPE`</span></td><td>The token's simple and extended part-of-speech tag, dependency label, lemma, shape</td></tr>\n",
    "<tr ><td><span >`ENT_TYPE`</span></td><td>The token's entity label</td></tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e43e28f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7125267737722935896, 1, 3)]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Tweet #100DaysOfCode')\n",
    "pattern1 = [{'ORTH': '#'}, {}]\n",
    "matcher.add('HashTags', [pattern1])\n",
    "found_matches = matcher(doc)\n",
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99fab0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c16a825",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf73d76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reaganomics.txt') as f:\n",
    "    doc3 = nlp(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c1659ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_list = ['voodoo economics', 'supply-side economics', 'trickle-down economics', 'free-market economics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b3de382",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_patterns = [nlp(text) for text in phrase_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d89d771",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add('EconMatcher', [*phrase_patterns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e548162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_matches = matcher(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d13dfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3680293220734633682 EconMatcher 41 45 supply-side economics\n",
      "3680293220734633682 EconMatcher 49 53 trickle-down economics\n",
      "3680293220734633682 EconMatcher 54 56 voodoo economics\n",
      "3680293220734633682 EconMatcher 61 65 free-market economics\n",
      "3680293220734633682 EconMatcher 673 677 supply-side economics\n",
      "3680293220734633682 EconMatcher 2987 2991 trickle-down economics\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in found_matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
    "    span = doc3[start:end]                    # get the matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bf30556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "REAGANOMICS\n",
       "https://en.wikipedia.org/wiki/Reaganomics\n",
       "\n",
       "Reaganomics (a portmanteau of [Ronald] Reagan and economics attributed to Paul Harvey)[1] refers to the economic policies promoted by U.S. President Ronald Reagan during the 1980s. These policies are commonly associated with supply-side economics, referred to as trickle-down economics or voodoo economics by political opponents, and free-market economics by political advocates.\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc3[:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "281ad10d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "same time he attracted a following from the supply-side economics movement, which formed in opposition to Keynesian"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc3[665:685]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
